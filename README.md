# ü§ñ RAG Chatbot - Full Stack Application

A **production-ready** Retrieval-Augmented Generation (RAG) chatbot with **FastAPI microservices backend** and **Next.js frontend**. This system processes documents, stores them in a vector database, performs intelligent retrieval, and generates contextual responses using large language models.

**‚ú® Recently Updated**: All critical production issues have been resolved, including TensorFlow compatibility, dependency conflicts, source attribution, and semantic search optimization.

[![Python](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![Qdrant](https://img.shields.io/badge/Qdrant-Vector%20DB-red.svg)](https://qdrant.tech/)
[![OpenAI](https://img.shields.io/badge/OpenAI-Embeddings%20%26%20LLM-orange.svg)](https://openai.com/)
[![Docker](https://img.shields.io/badge/Docker-Containerized-blue.svg)](https://www.docker.com/)
[![Tests](https://img.shields.io/badge/Tests-Production%20Validated-brightgreen.svg)](#testing)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-success.svg)](#status)

## üöÄ Quick Start

### Prerequisites
- Python 3.12+
- Docker and Docker Compose
- OpenAI API key

### Installation

1. **Clone and Setup**
   ```bash
   git clone https://github.com/[username]/fastapi-rag-microservices-application.git
   cd fastapi-rag-microservices-application
   
   # Create virtual environment
   uv venv
   source .venv/bin/activate  # Linux/Mac
   
   # Install dependencies
   uv pip install -r requirements.txt
   ```

2. **Configure Environment**
   ```bash
   cp .env.example .env
   # Edit .env with your OpenAI API key
   ```

3. **Start Services**
   ```bash
   # Quick start with Docker Compose
   make build
   make start
   
   # Or start individual services
   docker run -d --name qdrant -p 6333:6333 qdrant/qdrant:latest
   ```

4. **Verify Installation**
   ```bash
   # Check all services
   curl http://localhost:8000/health/all
   
   # Upload a document
   curl -X POST -F "file=@document.pdf" http://localhost:8000/upload
   
   # Chat with your documents
   curl -X POST -H "Content-Type: application/json" \
        -d '{"message": "What is this document about?"}' \
        http://localhost:8000/chat
   ```

## üèó Architecture

This system implements a **microservices architecture** with 5 core services:

```mermaid
graph TB
    Client[Client Application] --> Gateway[API Gateway<br/>:8000]
    
    Gateway --> Storage[Storage Service<br/>:8001]
    Gateway --> Retriever[Retriever Service<br/>:8002]
    Gateway --> Enhancement[Query Enhancement<br/>:8003]
    Gateway --> Generation[Generation Service<br/>:8004]
    
    Storage --> Qdrant[(Qdrant Vector DB<br/>:6333)]
    Retriever --> Qdrant
    
    Storage --> OpenAI[OpenAI API<br/>Embeddings]
    Generation --> OpenAI2[OpenAI API<br/>LLM]
```

### üîß Services Overview

| Service | Port | Purpose | Status | Recent Fixes |
|---------|------|---------|--------|-------------|
| **API Gateway** | 8000 | Request orchestration, chat pipeline | üü¢ **Production Ready** | Fixed attribute access, optimized thresholds |
| **Storage Service** | 8001 | File processing, chunking, embedding | üü¢ **Production Ready** | Fixed source attribution, TensorFlow compatibility |
| **Retriever Service** | 8002 | Hybrid search (semantic + keyword) | üü¢ **Production Ready** | Added rank_bm25, fixed imports |
| **Query Enhancement** | 8003 | Query preprocessing, intent classification | üü¢ **Functional** | Resolved compatibility issues |
| **Generation Service** | 8004 | LLM response generation | üü¢ **Functional** | Fixed source filtering |
| **Qdrant Database** | 6333 | Vector storage and similarity search | üü¢ **Production Ready** | CV storage verified |
| **Frontend App** | 3000 | Next.js web interface | üü¢ **Functional** | File upload and chat working |

## üìä Key Features

### üéØ **Intelligent Document Processing**
- **Multi-format Support**: PDF, DOCX, TXT, JSON
- **Smart Chunking**: Overlap-based text segmentation
- **Vector Embeddings**: OpenAI text-embedding-3-small with fallback
- **‚ú® Fixed Source Attribution**: Proper metadata preservation and tracking
- **‚ú® TensorFlow Compatibility**: Resolved Keras 3 conflicts

### üîç **Advanced Search Capabilities**
- **Semantic Search**: Vector similarity with optimized thresholds
- **Keyword Search**: BM25 algorithm for exact matching  
- **Hybrid Search**: Combined approach for optimal results
- **‚ú® Optimized Retrieval**: Lowered threshold from 0.6 to 0.1 for better coverage
- **‚ú® Real Document Validation**: CV storage and retrieval confirmed

### ü§ñ **AI-Powered Generation**
- **Context-Aware Responses**: Using retrieved document chunks
- **‚ú® Enhanced Source Attribution**: Clean citation handling
- **Conversation Memory**: Multi-turn chat support
- **Multiple LLM Support**: OpenAI GPT-4 with streaming
- **‚ú® Production Tested**: End-to-end validation with real documents

### üöÄ **Production-Ready Architecture**
- **Containerized Deployment**: Docker and Docker Compose
- **‚ú® Fixed Health Monitoring**: All services reachable via Gateway
- **Async Processing**: High-performance async/await
- **‚ú® Robust Error Handling**: Comprehensive exception management
- **‚ú® Frontend Integration**: Functional Next.js web application

## üìà Performance

### Real-World Test Results ‚ú® Recently Validated

| Metric | Performance | Details |
|--------|-------------|---------|
| **Document Processing** | ~1-2 seconds | PDF extraction + chunking + embedding |
| **Vector Storage** | 333+ chunks | Real documents + CV successfully stored |
| **Semantic Search** | 0.746 max score | High relevance for domain queries |
| **Hybrid Search** | ~300-700ms | Combined semantic + keyword results |
| **Full RAG Pipeline** | ~1-2 seconds | End-to-end query to response |
| **‚ú® CV Document Testing** | 4 chunks stored | "Yahya Khan" properly retrieved |
| **‚ú® Service Health** | All services up | Gateway accessible at :8000 |
| **‚ú® Frontend Integration** | Fully functional | Upload + chat working at :3000 |

### Scalability
- **Horizontal Scaling**: Each service independently scalable
- **Vector Database**: Qdrant supports distributed deployment
- **Load Balancing**: Ready for production proxy integration
- **Caching**: Redis integration planned for frequent queries

## üß™ Testing

### Test Coverage: Production Validated ‚ú®

#### **Storage Service** (Production Ready)
- ‚úÖ PDF/DOCX/TXT/JSON file processing
- ‚úÖ Text chunking with proper overlap
- ‚úÖ OpenAI embedding generation with fallback
- ‚úÖ Qdrant vector storage
- ‚úÖ Real document validation (26K + 121K characters)
- ‚ú® **Source attribution bug fixed**
- ‚ú® **TensorFlow compatibility resolved**

#### **Retriever Service** (Production Ready)
- ‚úÖ Qdrant integration with 333+ real document chunks
- ‚úÖ Semantic search scores: 0.538-0.746
- ‚úÖ BM25 keyword search functionality
- ‚úÖ Hybrid search combining both approaches
- ‚úÖ Multi-document source diversity
- ‚ú® **rank_bm25 dependency added**
- ‚ú® **sentence_transformers import fixed**

#### **End-to-End Production Testing** ‚ú®
- ‚úÖ Complete RAG pipeline with real CV document
- ‚úÖ Frontend file upload and chat functionality
- ‚úÖ All microservices health monitoring
- ‚úÖ Document storage verification in Qdrant
- ‚úÖ Query optimization and threshold tuning
- ‚úÖ Source attribution in generated responses

### Run Tests
```bash
source .venv/bin/activate

# Run all tests
python -m pytest tests/ -v

# Test specific components
python -m pytest tests/test_file_processor.py -v
python -m pytest tests/test_embedding_service.py -v

# Test with real data
python test_retriever_simple.py
```

## üìö API Documentation

### Core Endpoints

#### **Chat Interface**
```bash
# Complete RAG chat pipeline
POST /chat
{
  "message": "What is LoRA adaptation?",
  "conversation_id": "optional-uuid",
  "temperature": 0.7
}
```

#### **Document Management**
```bash
# Upload documents
POST /upload
Content-Type: multipart/form-data

# List stored files
GET /files

# Delete documents
DELETE /files/{file_id}
```

#### **Search Interface**
```bash
# Hybrid search
POST /search
{
  "query": "parameter efficient tuning",
  "top_k": 5,
  "threshold": 0.7
}
```

### Health Monitoring
```bash
# Check all services
GET /health/all

# Individual service health
GET /health
```

## üîß Configuration

### Environment Variables
```bash
# Required
OPENAI_API_KEY=your_openai_api_key_here

# Optional (with defaults)
QDRANT_HOST=localhost
QDRANT_PORT=6333
EMBEDDING_MODEL=text-embedding-3-small
LLM_MODEL=gpt-4o-mini
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# ‚ú® Recently Optimized
SEARCH_THRESHOLD=0.1  # Lowered from 0.6 for better retrieval
HYBRID_SEARCH_ENABLED=true
SOURCE_ATTRIBUTION=true  # Fixed in production
```

### Service Configuration
Each service is independently configurable through:
- Environment variables
- `.env` file
- `shared/config.py` defaults

## üöÄ Deployment

### Development
```bash
# Start all services
make build && make start

# View logs
make logs

# Stop services  
make stop

# ‚ú® Start frontend (tested and working)
cd frontend && npm run dev  # Runs on :3000
```

### Production ‚ú® Recently Validated
```bash
# Docker Compose deployment
docker-compose up -d

# Health check (all services now reachable)
curl http://localhost:8000/health/all

# Test document upload
curl -X POST -F "file=@your_cv.pdf" http://localhost:8000/upload

# Test chat with your documents
curl -X POST -H "Content-Type: application/json" \
     -d '{"message": "Who is mentioned in the document?"}' \
     http://localhost:8000/chat

# Scale services
docker-compose up -d --scale retriever-service=3
```

### CI/CD Pipeline (In Progress)
- GitHub Actions workflows
- Automated testing and deployment
- Security scanning and quality checks

## üìä Project Structure

```
fastapi-rag-microservices-application/
‚îú‚îÄ‚îÄ backend/                    # ‚ú® Backend microservices
‚îÇ   ‚îú‚îÄ‚îÄ services/               # Microservice implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ storage/            # Document processing (PRODUCTION READY)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ retriever/          # Search and retrieval (PRODUCTION READY)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ query_enhancement/  # Query preprocessing (FUNCTIONAL)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generation/         # LLM response generation (FUNCTIONAL)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gateway/            # API gateway (PRODUCTION READY)
‚îÇ   ‚îú‚îÄ‚îÄ shared/                 # Common models and utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py          # Pydantic data models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py           # Shared utility functions
‚îÇ   ‚îú‚îÄ‚îÄ tests/                  # Comprehensive test suite
‚îÇ   ‚îú‚îÄ‚îÄ logs/                   # Service logs
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml     # Backend orchestration
‚îú‚îÄ‚îÄ frontend/                   # ‚ú® Next.js web application (FUNCTIONAL)
‚îÇ   ‚îú‚îÄ‚îÄ src/                    # React components and pages
‚îÇ   ‚îú‚îÄ‚îÄ public/                 # Static assets
‚îÇ   ‚îú‚îÄ‚îÄ package.json            # Dependencies
‚îÇ   ‚îî‚îÄ‚îÄ next.config.js          # Next.js configuration
‚îú‚îÄ‚îÄ .github/                    # ‚ú® CI/CD workflows (IN PROGRESS)
‚îÇ   ‚îî‚îÄ‚îÄ workflows/              # GitHub Actions
‚îú‚îÄ‚îÄ docs/                       # Additional documentation
‚îú‚îÄ‚îÄ scripts/                    # Utility and setup scripts
‚îú‚îÄ‚îÄ TASKS.md                    # ‚ú® Updated development roadmap
‚îî‚îÄ‚îÄ README.md                  # This file
```

## üõ† Development

### Prerequisites for Development
- Python 3.12+
- Docker
- uv (fast Python package manager)
- OpenAI API key

### Development Workflow
1. **Setup**: `uv venv && source .venv/bin/activate`
2. **Install**: `uv pip install -r requirements.txt`
3. **Configure**: Copy `.env.example` to `.env`
4. **Test**: `python -m pytest tests/ -v`
5. **Run**: `make build && make start`

### Adding New Features
1. Implement in appropriate service directory
2. Add tests in `tests/` directory
3. Update shared models if needed
4. Test integration with other services
5. Update documentation

## üìù Documentation

- **[TASKS.md](TASKS.md)** - ‚ú® **Updated** development roadmap and recent fixes
- **[CLAUDE.md](CLAUDE.md)** - Comprehensive project documentation
- **[TEST_SUMMARY.md](TEST_SUMMARY.md)** - Detailed test results
- **[RETRIEVER_TEST_RESULTS.md](RETRIEVER_TEST_RESULTS.md)** - Retrieval validation

### ‚ú® Recent Updates (Aug 23, 2025)
- **Production Issue Fixes**: All critical bugs resolved
- **End-to-End Validation**: CV upload and retrieval confirmed
- **Frontend Integration**: Next.js application fully functional
- **CI/CD Pipeline**: GitHub Actions workflows in progress

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality  
4. Ensure all tests pass
5. Update documentation
6. Submit a pull request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **FastAPI** for the excellent web framework
- **Qdrant** for high-performance vector search
- **OpenAI** for embeddings and language models
- **Python ecosystem** for amazing ML/AI libraries

## üîó Links

- **[API Documentation](http://localhost:8000/docs)** - ‚ú® **Available Now** (when services running)
- **[Frontend Application](http://localhost:3000)** - ‚ú® **Available Now** (Next.js interface)
- **[Health Monitoring](http://localhost:8000/health/all)** - ‚ú® **All services reachable**
- **[Updated Roadmap](TASKS.md)** - Recent fixes and future plans
- **[Technical Deep Dive](CLAUDE.md)** - Comprehensive documentation

### ‚ú® Quick Start Links
```bash
# Backend API (all services)
http://localhost:8000/docs

# Frontend Application 
http://localhost:3000

# Individual Services
http://localhost:8001/docs  # Storage
http://localhost:8002/docs  # Retriever
http://localhost:8003/docs  # Query Enhancement
http://localhost:8004/docs  # Generation

# Vector Database
http://localhost:6333/dashboard  # Qdrant
```